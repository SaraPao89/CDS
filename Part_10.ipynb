{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "In linear regression our predictors are linear functions $f : \\R^d\\to\\R$ each parameterized by a vector $\\bw\\in\\R^d$ of real coefficients. That is, $f(\\bx) = \\bw^{\\top}\\bx$.\n",
    "\n",
    "Given a training set $(\\bx_1,y_1),\\ldots,(\\bx_m,y_m) \\in \\R^d\\times\\R$, the linear regression predictor is the Empirical Risk Minimizer with respect to the square loss,\n",
    "$$\n",
    "    \\bwhat = \\argmin_{\\bw\\in\\R^d} \\sum_{t=1}^m \\big(\\bw^{\\top}\\bx_t - y_t\\big)^2\n",
    "$$\n",
    "Now let $\\bv = \\big(\\bw^{\\top}\\bx_1,\\ldots,\\bw^{\\top}\\bx_m\\big)$ and $\\by = (y_1,\\ldots,y_m)$. Then\n",
    "$$\n",
    "     \\sum_{t=1}^m \\big(\\bw^{\\top}\\bx_t - y_t\\big)^2 = \\norm{\\bv-\\by}^2.\n",
    "$$\n",
    "Since $\\bv = S\\bw$, where $S$ is a $m \\times d$ matrix such that $S^{\\top} = [\\bx_1,\\ldots,\\bx_m]$, we may also write\n",
    "$$\n",
    "    \\bwhat = \\argmin_{\\bw\\in\\R^d} \\norm{S\\bw-\\by}^2\n",
    "$$\n",
    "Since $F(\\bw) = \\norm{S\\bw-\\by}^2$ is a convex function, the minimizer satisfies the condition $\\nabla F(\\bw) = \\bzero$.\n",
    "\n",
    "Using matrix calculus, we have that $\\nabla\\norm{S\\bw-\\by}^2 = 2S^{\\top}(S\\bw-\\by)$. Hence, $\\nabla\\norm{S\\bw-\\by}^2 = \\bzero$ for $\\bw = \\big(S^{\\top}S\\big)^{-1}S^{\\top}\\by$ provided $S^{\\top}S$ is invertible.\n",
    "\n",
    "Therefore, whenever $S^{\\top}S$ is invertible we have that $\\bwhat = \\big(S^{\\top}S\\big)^{-1}S^{\\top}\\by$.\n",
    "\n",
    "Since the Bayes optimal predictor for square loss is $f(\\bx) = \\E[Y \\mid \\bx]$, the Bayes optimal predictor is a linear function when $y = \\bu^{\\top}\\bx + \\ve_{\\bx}$, where $\\ve_{\\bx}$ is an independent random variable such that $\\E[\\ve_{\\bx} \\mid \\bx] = 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import decomposition\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.model_selection import train_test_split, learning_curve, cross_val_score, validation_curve, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "#from auxCode import *\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by using the function `make_regression()` to generate a random dataset on which the Bayes optimal predictor is a linear function. We set $d=1$, $m=1000$, and $\\sqrt{\\Var[\\ve_{\\bx} \\mid \\bx]} = 10$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, w = make_regression(n_samples=1000, n_features=1, n_informative=1, n_targets=1,\n",
    "                          noise=10.0, shuffle=True, coef=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inspect the coefficient of the Bayes optimal predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(w.item(), decimals=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Bayes risk is the squared conditional variance, averaged over data points. In our case, Bayes risk $(10)^2 = 100$.\n",
    "\n",
    "If we compute it on our dataset we get something close but not quite the same, which is reasonable considering we have sampled just $1000$ data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_opt = w.item()*X\n",
    "np.round(mean_squared_error(y, y_opt), decimals=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we split the dataset in train and test, with proportions 60% and 40%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run a linear regression learner on the training data and print the learned coefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = LinearRegression()\n",
    "learner.fit(X_train, y_train)\n",
    "np.round(learner.coef_[0], decimals=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the learned linear regressor is pretty close to the coefficient $16.75$ of the Bayes optimal predictor.\n",
    "\n",
    "We then compute test predictions and measure test error with respect to the square loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = learner.predict(X_test)\n",
    "np.round(mean_squared_error(y_test, y_pred), decimals=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we plot the test points and the two linear regressors (the one we learned and the Bayes optimal one) we can get an idea of how good is linear regression when the Bayes optimal predictor is also a linear function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_min = np.min(X)\n",
    "x_max = np.max(X)\n",
    "v = np.arange(x_min, x_max, 0.2)\n",
    "\n",
    "plt.scatter(X_test, y_test,  color='yellow')\n",
    "plt.plot(X_test, y_pred, color='blue', linewidth=2)\n",
    "plt.plot(v, v*w.item(), color='red', linewidth=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test error of the Bayes optimal predictor is only slightly lower than that of the linear regressor learned on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_opt = w.item()*X_test\n",
    "np.round(mean_squared_error(y_test, y_opt), decimals=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we keep the same training set and change the labels to make the Bayes optimal predictor a nonlinear function of $x$, namely $y = \\sin(x) + 0.3\\ve_x$, where $\\ve_x$ is an independent Gaussian random variable with zero mean and variance equal to $0.25$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y = X * np.sin(X) + 0.5*np.random.randn(1000,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X, y,  color='yellow')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we train on the entire dataset, linear regression achieves a training error of $0.58$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.fit(X, y)\n",
    "y_pred = learner.predict(X)\n",
    "np.round(mean_squared_error(y, y_pred), decimals=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now use a trick to make linear regression more powerful at the expense of increasing the number of features.\n",
    "\n",
    "The function `PolynomialFeatures()` replaces the original features $\\bx = (x_1,\\ldots,x_d)$ with all polynomial combinations of features with degree less than or equal to a specified degree $n$. For instance, if $d=2$ and $n=2$, the original features $(x_1,x_2)$ are replaced by $(1,x_1,x_2,x_1x_2,x_1^2,x_2^2)$. Note that the number of new features is of order $d^n$.\n",
    "\n",
    "With $n=5$ the training error of linear regression goes down a lot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = PolynomialFeatures(degree=5, interaction_only=False, include_bias=True)\n",
    "X_poly = poly.fit_transform(X)\n",
    "learner.fit(X_poly, y)\n",
    "y_pred = learner.predict(X_poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(mean_squared_error(y, y_pred), decimals=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X, y,  color='yellow')\n",
    "plt.scatter(X, y_pred,  color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to try out linear regression on a real-world dataset.\n",
    "\n",
    "The Boston Housing Dataset originates from the UCI Machine Learning Repository. The data was collected in $1978$ and each of the $506$ entries represent aggregated data about $14$ features for homes from various suburbs in Boston, Massachusetts. The label is the column `MEDV`.\n",
    "\n",
    "We load the dataset and add column names. Note the the field separator in the file is whitespace instead of comma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n",
    "housing = pd.read_csv(\"Datasets/housing.csv\", delim_whitespace=True, names=names)\n",
    "housing.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take a look at the distribution of the labels using the `distplot()` function of Seaborn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.distplot(housing['MEDV']);\n",
    "fig = plt.figure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also look at the correlation matrix between features. The DataFrame method `corr()` of Pandas computes the Pearson correlation coefficient of the sample, defined for each pair $i,j$ of features by\n",
    "$$\n",
    "    r_{i,j} = \\frac{\\sum_{t=1}^m (x_{i,t}-\\mu_i)(x_{j,t}-\\mu_j)}{\\sqrt{\\sum_{t=1}^m (x_{i,t}-\\mu_i)^2}\\sqrt{\\sum_{t=1}^m (x_{j,t}-\\mu_j)^2}}\n",
    "$$\n",
    "where $\\mu_i = (x_{i,1}+\\cdots+x_{i,m})/m$.\n",
    "\n",
    "Recall that the Pearson correlation coefficient $r_{i,j}$ is always in the $[-1,+1]$ interval. Values of $r_{i,j}$ closer to the extremes indicate that $i$ and $j$ tend to be linearly correlated, whereas values of $r_{i,j}$ close to zero indicate that $i$ and $j$ are nearly independent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr=housing.corr()\n",
    "plt.figure(figsize=(20, 20))\n",
    "sns.heatmap(corr, vmax=.8, linewidths=0.01,\n",
    "            square=True,annot=True,cmap='YlGnBu',linecolor=\"white\")\n",
    "plt.title('Correlation between features');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pairs of features with correlation coefficient close to $-1$ or $+1$ are redundant (for example, `TAX` and `RAD` in this dataset), and we could only keep one for each pair.\n",
    "\n",
    "We check which feature has a correlation larger than $0.75$ in absolute value with at least some other feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr[((corr > 0.75) | (corr < -0.75)) & (corr != 1.0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that we can safely drop `NOX` and `RAD`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = housing.drop(columns=['NOX', 'RAD', 'MEDV']).values\n",
    "y = housing['MEDV'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now set the linear regression learner with the (negative) square loss as scoring function. In our experiments, use the setting `normalize=True` which applies the data transformation $\\bx_t \\to (\\bx_t - \\bmu)/\\norm{\\bx_t}$ where $\\bmu$ is the average of the data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = LinearRegression(normalize=True)\n",
    "RMS = 'neg_mean_squared_error'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cross-validated risk estimate is $34.77$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(learner, X, y, cv=5, scoring=RMS)\n",
    "np.round(-scores.mean(), decimals=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now look at the learning curve for training set values from $150$ to $300$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = range(150, 301, 25)\n",
    "train_size, train_score, val_score = learning_curve(learner, X, y, train_sizes=sizes, cv=5, scoring=RMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_score_mean = -np.mean(train_score, axis=1)\n",
    "train_score_std = np.std(train_score, axis=1)\n",
    "val_score_mean = -np.mean(val_score, axis=1)\n",
    "val_score_std = np.std(val_score, axis=1)\n",
    "plt.grid()\n",
    "plt.fill_between(sizes, train_score_mean - train_score_std,\n",
    "                 train_score_mean + train_score_std, alpha=0.1,\n",
    "                 color=\"r\")\n",
    "plt.fill_between(sizes, val_score_mean - val_score_std,\n",
    "                 val_score_mean + val_score_std, alpha=0.1, color=\"g\")\n",
    "plt.plot(sizes, train_score_mean, 'o-', color=\"r\",\n",
    "         label=\"Training error\")\n",
    "plt.plot(sizes, val_score_mean, 'o-', color=\"g\",\n",
    "         label=\"CV error\")\n",
    "plt.legend()\n",
    "plt.xlabel('Training size')\n",
    "plt.ylabel('Square loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression look **unstable** here. The risk estimates oscillate between $40$ and $80$ and have a huge standard deviation. \n",
    "\n",
    "We repeat the experiment without cross-validation so that we can extract the linear model $\\bwhat$ learned by linear regression for each training set size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_list = []\n",
    "\n",
    "for s in range(150, 301, 25):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=s, random_state=42)\n",
    "    learner.fit(X_train, y_train)\n",
    "    coef_list.append(learner.coef_)\n",
    "\n",
    "coef_matrix = np.array(coef_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we use PCA to visually check how different the learned models are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = decomposition.PCA(n_components=2)\n",
    "pca.fit(coef_matrix)\n",
    "coef_pca = pca.transform(coef_matrix)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlim(-1.5, 1.5)\n",
    "ax.set_ylim(-0.5, 0.5)\n",
    "plt.scatter(coef_pca[:,0], coef_pca[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to increase the stability of linear regression, we first use PCA to reduce the number of features and increase the bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = decomposition.PCA(n_components=11)\n",
    "pca.fit(X)\n",
    "\n",
    "plt.title('PCA')\n",
    "plt.plot(pca.singular_values_, label='Singular values')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We project all the datapoints on the six principal components. Then we check the learning curve again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = decomposition.PCA(n_components=6)\n",
    "pca.fit(X)\n",
    "X_pca = pca.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = range(150, 301, 25)\n",
    "train_size, train_score, val_score = learning_curve(learner, X_pca, y, train_sizes=sizes, cv=5, scoring=RMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('PCA Linear Regression')\n",
    "train_score_mean = -np.mean(train_score, axis=1)\n",
    "train_score_std = np.std(train_score, axis=1)\n",
    "val_score_mean = -np.mean(val_score, axis=1)\n",
    "val_score_std = np.std(val_score, axis=1)\n",
    "plt.grid()\n",
    "plt.fill_between(sizes, train_score_mean - train_score_std,\n",
    "                 train_score_mean + train_score_std, alpha=0.1,\n",
    "                 color=\"r\")\n",
    "plt.fill_between(sizes, val_score_mean - val_score_std,\n",
    "                 val_score_mean + val_score_std, alpha=0.1, color=\"g\")\n",
    "plt.plot(sizes, train_score_mean, 'o-', color=\"r\",\n",
    "         label=\"Training error\")\n",
    "plt.plot(sizes, val_score_mean, 'o-', color=\"g\",\n",
    "         label=\"CV risk estimate\")\n",
    "plt.legend()\n",
    "plt.xlabel('Training size')\n",
    "plt.ylabel('Square loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learning curve looks more stable now. We doublecheck by repeating the procedure to plot the spread of the vectors learned using increasing sizes of the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_list = []\n",
    "\n",
    "for s in range(150, 301, 25):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_pca, y, train_size=s, random_state=42)\n",
    "    learner.fit(X_train, y_train)\n",
    "    coef_list.append(learner.coef_)\n",
    "coef_matrix = np.array(coef_list)\n",
    "\n",
    "pca = decomposition.PCA(n_components=2)\n",
    "pca.fit(coef_matrix)\n",
    "coef_pca = pca.transform(coef_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.set_xlim(-1.5, 1.5)\n",
    "ax.set_ylim(-0.5, 0.5)\n",
    "plt.scatter(coef_pca[:,0], coef_pca[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the plot shows, the learned models are a indeed more stable after PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A better way to increase the bias in linear regression, and thus make the model more stable, is by introducing a regularizer in the ERM functional.\n",
    "\n",
    "In other words, instead defining $\\bwhat$ by\n",
    "$$\n",
    "    \\bwhat = \\argmin_{\\bw\\in\\R^d} \\norm{S\\bw-\\by}^2\n",
    "$$\n",
    "we use the regularized form, also known as *Ridge Regression*,\n",
    "$$\n",
    "    \\bwhat = \\argmin_{\\bw\\in\\R^d} \\norm{S\\bw-\\by}^2 + \\alpha\\norm{\\bw}^2\n",
    "$$\n",
    "where $\\alpha \\ge 0$ is the regularization parameter. When $\\alpha$ is zero we recover the standard linear regression solution. When $\\alpha$ is very large, the solution $\\bwhat$ becomes the zero vector. Hence $\\alpha$ can be used to control the bias of the algorithm.\n",
    "\n",
    "Similarly to before, we have that\n",
    "$$\n",
    "    \\nabla\\Big(\\norm{S\\bw-\\by}^2 + \\alpha\\norm{\\bw}^2\\Big) = 2S^{\\top}(S\\bw-\\by) + 2\\alpha\\bw\n",
    "$$    \n",
    "Hence, the gradient vanishes for $\\bw = \\big(\\alpha I + S^{\\top}S\\big)^{-1}S^{\\top}\\by$. Note that we do not have to worry anymore about the invertibility of $S^{\\top}S$. Indeed, if $\\lambda_1 \\ge\\cdots\\ge \\lambda_d \\ge 0$ are the eigenvalues of $S^{\\top}S$, the eigenvalues of $\\alpha I + S^{\\top}S$ are simply\n",
    "$\n",
    "    \\alpha+\\lambda_1 \\ge\\cdots\\ge \\alpha+\\lambda_d > 0\n",
    "$. Hence, $\\alpha I + S^{\\top}S$ is invertible whenever $\\alpha > 0$.\n",
    "\n",
    "We start by investigating the sensitivity of the CV risk estimate to the choice of $\\alpha$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_vals = np.linspace(0.01,2.0,20)\n",
    "train_score, val_score = validation_curve(Ridge(normalize=True), X, y, 'alpha', alpha_vals, cv=5, scoring=RMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Ridge Regression')\n",
    "train_score_mean = -np.mean(train_score, axis=1)\n",
    "train_score_std = np.std(train_score, axis=1)\n",
    "val_score_mean = -np.mean(val_score, axis=1)\n",
    "val_score_std = np.std(val_score, axis=1)\n",
    "plt.grid()\n",
    "plt.fill_between(alpha_vals, train_score_mean - train_score_std,\n",
    "                 train_score_mean + train_score_std, alpha=0.1,\n",
    "                 color=\"r\")\n",
    "plt.fill_between(alpha_vals, val_score_mean - val_score_std,\n",
    "                 val_score_mean + val_score_std, alpha=0.1, color=\"g\")\n",
    "plt.plot(alpha_vals, train_score_mean, color=\"r\",\n",
    "         label=\"Training error\")\n",
    "plt.plot(alpha_vals, val_score_mean, color=\"g\",\n",
    "         label=\"CV risk estimate\")\n",
    "plt.legend()\n",
    "plt.xlabel('Alpha')\n",
    "plt.ylabel('Square loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that overfitting essentially disappears for $\\alpha > 0.5$, and bias progressively increases. The variance of the CV risk estimate remains high though.\n",
    "\n",
    "With normalized data (as performed by the flag `normalize=True`), the choice $\\alpha = 1$ is often reasonable.\n",
    "\n",
    "We now compare the learning curves of PCA Linear Regression and Ridge Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg = LinearRegression(normalize=True)\n",
    "\n",
    "sizes = range(150, 301, 25)\n",
    "l_train_size, l_train_score, l_val_score = learning_curve(lin_reg,\n",
    "                                                          X_pca, y, train_sizes=sizes, cv=5, scoring=RMS)\n",
    "r_train_size, r_train_score, r_val_score = learning_curve(Ridge(normalize=True),\n",
    "                                                          X_pca, y, train_sizes=sizes, cv=5, scoring=RMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('PCA Linear Regression vs. Ridge Regression')\n",
    "l_val_score_mean = -np.mean(l_val_score, axis=1)\n",
    "l_val_score_std = np.std(l_val_score, axis=1)\n",
    "r_val_score_mean = -np.mean(r_val_score, axis=1)\n",
    "r_val_score_std = np.std(r_val_score, axis=1)\n",
    "plt.grid()\n",
    "plt.fill_between(sizes, l_val_score_mean - l_val_score_std,\n",
    "                 l_val_score_mean + l_val_score_std, alpha=0.1,\n",
    "                 color=\"r\")\n",
    "plt.fill_between(sizes, r_val_score_mean - r_val_score_std,\n",
    "                 r_val_score_mean + r_val_score_std, alpha=0.1, color=\"g\")\n",
    "plt.plot(sizes, l_val_score_mean, 'o-', color=\"r\",\n",
    "         label=\"PCA Linear Regression\")\n",
    "plt.plot(sizes, r_val_score_mean, 'o-', color=\"g\",\n",
    "         label=\"Ridge Regression\")\n",
    "plt.legend()\n",
    "plt.xlabel('Training size')\n",
    "plt.ylabel('CV risk estimate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that, despite a large variance in the CV risk estimate, Ridge Regression is better than PCA Linear Regression, especially in the small training set regime."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": true,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
